import csv
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
from vc_search.search.elastic_client import VCElasticSearch


class QueryEvaluator:
    def __init__(self, es_client: VCElasticSearch):
        self.es = es_client
        self.queries = [
            "–ò–ò –≤ —Ç—Ä–µ–π–¥–∏–Ω–≥–µ",
            "–Ω–æ–≤—ã–π VR —à–ª–µ–º Valve",
            "Telegram –æ–±–Ω–æ–≤–∏–ª –¥–∏–∑–∞–π–Ω iOS",
            "–∫—Ä–∏–ø—Ç–æ—Ä—ã–Ω–æ–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ 2025",
            "—Å–æ–∑–¥–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ Sora",
            "–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ SIM –∫–∞—Ä—Ç —Ä–æ—É–º–∏–Ω–≥",
            "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –¥–∏–∑–∞–π–Ω–∞ –∏–Ω—Ç–µ—Ä—å–µ—Ä–∞",
            "–∑–∞—Ä–ø–ª–∞—Ç–Ω—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è –∑—É–º–µ—Ä–æ–≤",
            "–°–±–µ—Ä–ú–æ–±–∞–π–ª –ø–æ–¥–ø–∏—Å–∫–∞ –õ–∏—Ç—Ä–µ—Å",
            "–±—É–¥—É—â–µ–µ –±–∏–∑–Ω–µ—Å–∞ —ç–º–ø–∞—Ç–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
        ]

    def clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤"""
        if not text:
            return ""
        # –ó–∞–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏ —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        return " ".join(text.replace("\n", " ").replace("\r", " ").split())

    def execute_queries(self, results_per_query: int = 10) -> List[Dict[str, Any]]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        all_results = []

        for query in self.queries:
            print(f"üîç –í—ã–ø–æ–ª–Ω—è—é –∑–∞–ø—Ä–æ—Å: '{query}'")

            search_results = self.es.search_with_relevance_model(
                query, limit=results_per_query
            )

            for i, result in enumerate(search_results["results"]):
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º highlights
                highlights_text = ""
                if result.get("highlights"):
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ highlights –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
                    cleaned_highlights = [
                        self.clean_text(h) for h in result["highlights"]
                    ]
                    highlights_text = " | ".join(cleaned_highlights)

                record = {
                    "relevance": result.get(
                        "relevance_prediction", ""
                    ),  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º
                    "query": query,
                    "title": self.clean_text(result.get("title", "")),
                    "highlight": highlights_text,
                    "url": result.get("url", ""),
                    "author": result.get("author", ""),
                    "section": result.get("section", ""),
                    "score": result.get("score", 0),
                    "rank_position": i + 1,  # –ü–æ–∑–∏—Ü–∏—è –≤ –≤—ã–¥–∞—á–µ
                    "relevance_probability": result.get(
                        "relevance_probability", 0
                    ),  # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                }
                all_results.append(record)

            print(
                f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(search_results['results'])} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{query}'"
            )

        return all_results

    def save_to_csv(self, results: List[Dict[str, Any]], output_file: str = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV —Ñ–∞–π–ª"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"search_results_ml_{timestamp}.csv"

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        fieldnames = [
            "relevance",
            "query",
            "title",
            "highlight",
            "url",
            "author",
            "section",
            "score",
            "rank_position",
            "relevance_probability",
        ]

        with open(output_path, "w", newline="", encoding="utf-8-sig") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            for result in results:
                writer.writerow(result)

        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
        print(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(results)}")

        return output_path


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    print("üéØ –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞ vc.ru —Å ML –º–æ–¥–µ–ª—å—é")
    print("=" * 50)

    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Elasticsearch
    es = VCElasticSearch()

    if not es.health_check():
        print("‚ùå Elasticsearch –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω!")
        print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ: docker-compose -f docker-compose.elastic.yml up -d")
        return

    print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Elasticsearch —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")

    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    model_loaded = False
    model_path = "../../../data/models/lr_model.pkl"

    if Path(model_path).exists():
        try:
            es.load_relevance_model(model_path)
            model_loaded = True
            print("‚úÖ –ú–æ–¥–µ–ª—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {e}")
            print("ü§ñ –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫")
    else:
        print(f"‚ö†Ô∏è –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
        print("ü§ñ –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫")

    # –°–æ–∑–¥–∞–µ–º evaluator –∏ –≤—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å—ã
    evaluator = QueryEvaluator(es)
    results = evaluator.execute_queries(results_per_query=10)

    if not results:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞")
        return

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    output_file = evaluator.save_to_csv(results)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏:")
    print(f"   –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(evaluator.queries)}")
    print(f"   –í—Å–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    relevant_count = sum(1 for r in results if r.get("relevance") == 1)
    print(f"   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–º–µ—á–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {relevant_count}/{len(results)}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
    queries_stats = {}
    for result in results:
        query = result["query"]
        if query not in queries_stats:
            queries_stats[query] = {"total": 0, "relevant": 0}
        queries_stats[query]["total"] += 1
        if result.get("relevance") == 1:
            queries_stats[query]["relevant"] += 1

    print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º:")
    for query, stats in queries_stats.items():
        rel_percent = (
            (stats["relevant"] / stats["total"]) * 100 if stats["total"] > 0 else 0
        )
        print(
            f"   '{query}': {stats['relevant']}/{stats['total']} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö ({rel_percent:.1f}%)"
        )

    print(f"\nüéâ –§–∞–π–ª {output_file} —Å–æ–∑–¥–∞–Ω!")
    if model_loaded:
        print("   –°—Ç–æ–ª–±–µ—Ü 'relevance' –∑–∞–ø–æ–ª–Ω–µ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ ML –º–æ–¥–µ–ª–∏")
        print("   –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –≤—Ä—É—á–Ω—É—é")
    else:
        print("   ‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ –±–µ–∑ ML –º–æ–¥–µ–ª–∏")
        print(
            "   –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ML –º–æ–¥–µ–ª–∏ –Ω—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –≤ data/models/relevance_model.pkl"
        )


if __name__ == "__main__":
    main()
