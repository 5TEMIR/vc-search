import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from pathlib import Path


class RelevanceAutoMarker:
    def __init__(self, max_features=5000, min_df=2, max_df=0.8, ngram_range=(1, 2)):
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            min_df=min_df,
            max_df=max_df,
            ngram_range=ngram_range,
            stop_words=["–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–æ", "–¥–ª—è", "–Ω–µ", "—á—Ç–æ", "—ç—Ç–æ"],
        )

    def preprocess_text(self, text):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not isinstance(text, str):
            return ""

        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É –∏ –±–∞–∑–æ–≤—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        text = re.sub(r"[^–∞-—è—ëa-z0-9\s\.\,\!\?]", " ", text.lower())
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r"\s+", " ", text).strip()
        return text

    def calculate_relevance_scores(self, queries, contents):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º"""
        try:
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
            processed_queries = [self.preprocess_text(q) for q in queries]
            processed_contents = [self.preprocess_text(c) for c in contents]

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            all_texts = processed_queries + processed_contents

            # –°–æ–∑–¥–∞–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É
            tfidf_matrix = self.vectorizer.fit_transform(all_texts)

            # –†–∞–∑–¥–µ–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã –∏ –∫–æ–Ω—Ç–µ–Ω—Ç
            query_vectors = tfidf_matrix[: len(queries)]
            content_vectors = tfidf_matrix[len(queries) :]

            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –∑–∞–ø—Ä–æ—Å-–∫–æ–Ω—Ç–µ–Ω—Ç
            similarities = cosine_similarity(query_vectors, content_vectors)

            # –ë–µ—Ä–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å (–∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º)
            scores = [similarities[i][i] for i in range(len(queries))]
            return scores

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {e}")
            return [0.0] * len(queries)

    def mark_csv_file(self, input_csv, output_csv=None, threshold=None):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–º–µ—á–∞–µ—Ç CSV —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞"""
        if output_csv is None:
            input_path = Path(input_csv)
            output_csv = f"{input_path.stem}_marked{input_path.suffix}"

        # –ß–∏—Ç–∞–µ–º CSV —Ñ–∞–π–ª
        print(f"üìñ –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {input_csv}")
        df = pd.read_csv(input_csv)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = ["query", "article_content"]
        for col in required_columns:
            if col not in df.columns:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞: {col}")

        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫")

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        df["query"] = df["query"].fillna("")
        df["article_content"] = df["article_content"].fillna("")

        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        print("üîç –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏...")
        queries = df["query"].tolist()
        contents = df["article_content"].tolist()

        scores = self.calculate_relevance_scores(queries, contents)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ scores –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞
        scores_array = np.array(scores)

        print(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏:")
        print(f"   –ú–∏–Ω–∏–º—É–º: {scores_array.min():.4f}")
        print(f"   –ú–∞–∫—Å–∏–º—É–º: {scores_array.max():.4f}")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ: {scores_array.mean():.4f}")
        print(f"   –ú–µ–¥–∏–∞–Ω–∞: {np.median(scores_array):.4f}")
        print(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {scores_array.std():.4f}")

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω
        if threshold is None:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–≤–∞–Ω—Ç–∏–ª–µ–π
            threshold = np.percentile(scores_array, 70)  # 70-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å
            print(f"üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω –ø–æ—Ä–æ–≥: {threshold:.4f}")
        else:
            print(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–∞–¥–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥: {threshold:.4f}")

        # –†–∞–∑–º–µ—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ - —Ç–æ–ª—å–∫–æ 1 –∏–ª–∏ 0
        df["relevance_score"] = scores
        df["relevance"] = (scores_array >= threshold).astype(int)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–∑–º–µ—Ç–∫–∏
        relevant_count = df["relevance"].sum()
        relevant_percent = (relevant_count / len(df)) * 100

        print(f"‚úÖ –†–∞–∑–º–µ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        print(
            f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö (1): {relevant_count}/{len(df)} ({relevant_percent:.1f}%)"
        )
        print(
            f"   –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö (0): {len(df) - relevant_count}/{len(df)} ({(100 - relevant_percent):.1f}%)"
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ —Å—Ç–æ–ª–±—Ü–µ relevance —Ç–æ–ª—å–∫–æ 0 –∏ 1
        unique_values = df["relevance"].unique()
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ relevance: {sorted(unique_values)}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        df.to_csv(output_csv, index=False, encoding="utf-8")
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_csv}")

        return df, output_csv

    def analyze_results(self, df):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏"""
        print("\nüìä –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ —Å—Ç–æ–ª–±—Ü–µ relevance —Ç–æ–ª—å–∫–æ 0 –∏ 1
        unique_values = df["relevance"].unique()
        if set(unique_values) <= {0, 1}:
            print("‚úÖ –í —Å—Ç–æ–ª–±—Ü–µ 'relevance' —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏—è 0 –∏ 1")
        else:
            print(f"‚ö†Ô∏è  –í —Å—Ç–æ–ª–±—Ü–µ 'relevance' –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {unique_values}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
        query_stats = (
            df.groupby("query")
            .agg(
                {
                    "relevance": ["count", "sum", "mean"],
                    "relevance_score": ["mean", "std"],
                }
            )
            .round(3)
        )

        query_stats.columns = [
            "total",
            "relevant",
            "relevance_rate",
            "score_mean",
            "score_std",
        ]
        query_stats = query_stats.sort_values("relevance_rate", ascending=False)

        print("üìù –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º (—Ç–æ–ø-10 –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏):")
        print(query_stats.head(10).to_string())

        return query_stats


def main():
    import sys
    import argparse

    parser = argparse.ArgumentParser(
        description="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
    )
    parser.add_argument("input_csv", help="–í—Ö–æ–¥–Ω–æ–π CSV —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞")
    parser.add_argument(
        "--output", "-o", help="–í—ã—Ö–æ–¥–Ω–æ–π CSV —Ñ–∞–π–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: input_marked.csv)"
    )
    parser.add_argument(
        "--threshold", "-t", type=float, help="–ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (0-1)"
    )
    parser.add_argument(
        "--analyze",
        "-a",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
    )

    args = parser.parse_args()

    if not Path(args.input_csv).exists():
        print(f"‚ùå –§–∞–π–ª {args.input_csv} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    # –°–æ–∑–¥–∞–µ–º –º–∞—Ä–∫–µ—Ä –∏ —Ä–∞–∑–º–µ—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    marker = RelevanceAutoMarker()

    try:
        df, output_file = marker.mark_csv_file(
            input_csv=args.input_csv, output_csv=args.output, threshold=args.threshold
        )

        if args.analyze:
            marker.analyze_results(df)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return


if __name__ == "__main__":
    main()
