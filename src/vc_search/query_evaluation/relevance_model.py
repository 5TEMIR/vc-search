import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score
import re
from pathlib import Path
import joblib


class RelevanceModel:
    def __init__(self, model_type="random_forest"):
        self.model_type = model_type
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            min_df=2,
            max_df=0.8,
            ngram_range=(1, 2),
            stop_words=["–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–æ", "–¥–ª—è", "–Ω–µ", "—á—Ç–æ", "—ç—Ç–æ"],
        )
        self.model = None
        self.is_trained = False

    def preprocess_text(self, text):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not isinstance(text, str):
            return ""
        text = re.sub(r"[^–∞-—è—ëa-z0-9\s\.\,\!\?]", " ", text.lower())
        text = re.sub(r"\s+", " ", text).strip()
        return text

    def prepare_features(self, df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        queries = [self.preprocess_text(q) for q in df["query"]]
        contents = [self.preprocess_text(c) for c in df["article_content"]]

        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        combined_texts = [f"{q} {c}" for q, c in zip(queries, contents)]

        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç
        if self.is_trained:
            X = self.vectorizer.transform(combined_texts)
        else:
            X = self.vectorizer.fit_transform(combined_texts)

        return X

    def train(self, csv_file, test_size=0.2, random_state=42):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print(f"üìñ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {csv_file}")
        df = pd.read_csv(csv_file)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = ["query", "article_content", "relevance"]
        for col in required_columns:
            if col not in df.columns:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞: {col}")

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        df["query"] = df["query"].fillna("")
        df["article_content"] = df["article_content"].fillna("")
        df["relevance"] = df["relevance"].fillna(0).astype(int)

        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {df['relevance'].value_counts().to_dict()}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        X = self.prepare_features(df)
        y = df["relevance"].values

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )

        print(f"üéØ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: train={X_train.shape[0]}, test={X_test.shape[0]}")

        # –í—ã–±–æ—Ä –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if self.model_type == "random_forest":
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=random_state,
                class_weight="balanced",
            )
        elif self.model_type == "logistic_regression":
            self.model = LogisticRegression(
                random_state=random_state, class_weight="balanced", max_iter=1000
            )
        elif self.model_type == "svm":
            self.model = SVC(
                random_state=random_state, class_weight="balanced", probability=True
            )
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {self.model_type}")

        print(f"ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {self.model_type}...")
        self.model.fit(X_train, y_train)
        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        print("üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
        y_pred = self.model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å (accuracy): {accuracy:.4f}")
        print(f"üéØ F1-score: {f1:.4f}")
        print("\nüìã Classification Report:")
        print(classification_report(y_test, y_pred))

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è Random Forest)
        if hasattr(self.model, "feature_importances_"):
            feature_names = self.vectorizer.get_feature_names_out()
            importances = self.model.feature_importances_
            top_indices = np.argsort(importances)[-10:][::-1]

            print("\nüîù –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            for i, idx in enumerate(top_indices):
                print(f"   {i + 1}. {feature_names[idx]}: {importances[idx]:.4f}")

        return accuracy, f1

    def predict(self, query, content):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–¥–Ω–æ–π –ø–∞—Ä—ã –∑–∞–ø—Ä–æ—Å-–∫–æ–Ω—Ç–µ–Ω—Ç"""
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ –º–µ—Ç–æ–¥ train()")

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        processed_query = self.preprocess_text(query)
        processed_content = self.preprocess_text(content)
        combined_text = f"{processed_query} {processed_content}"

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        X = self.vectorizer.transform([combined_text])

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = self.model.predict(X)[0]
        probability = self.model.predict_proba(X)[0][1]

        return prediction, probability

    def predict_batch(self, queries, contents):
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ –º–µ—Ç–æ–¥ train()")

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        processed_queries = [self.preprocess_text(q) for q in queries]
        processed_contents = [self.preprocess_text(c) for c in contents]
        combined_texts = [
            f"{q} {c}" for q, c in zip(processed_queries, processed_contents)
        ]

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        X = self.vectorizer.transform(combined_texts)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions = self.model.predict(X)
        probabilities = self.model.predict_proba(X)[:, 1]

        return predictions, probabilities

    def save(self, model_path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")

        model_data = {
            "model_type": self.model_type,
            "vectorizer": self.vectorizer,
            "model": self.model,
            "is_trained": self.is_trained,
        }

        joblib.dump(model_data, model_path)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")

    def load(self, model_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        model_data = joblib.load(model_path)

        self.model_type = model_data["model_type"]
        self.vectorizer = model_data["vectorizer"]
        self.model = model_data["model"]
        self.is_trained = model_data["is_trained"]

        print(f"üì• –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
        print(f"ü§ñ –¢–∏–ø –º–æ–¥–µ–ª–∏: {self.model_type}")
        print(f"üìä –û–±—É—á–µ–Ω–∞: {self.is_trained}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
    parser.add_argument("input_csv", help="CSV —Ñ–∞–π–ª —Å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    parser.add_argument(
        "--model-type",
        "-m",
        choices=["random_forest", "logistic_regression", "svm"],
        default="random_forest",
        help="–¢–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è",
    )
    parser.add_argument(
        "--output",
        "-o",
        default="relevance_model.pkl",
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏",
    )
    parser.add_argument(
        "--test-size", "-t", type=float, default=0.2, help="–î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
    )

    args = parser.parse_args()

    if not Path(args.input_csv).exists():
        print(f"‚ùå –§–∞–π–ª {args.input_csv} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = RelevanceModel(model_type=args.model_type)

    try:
        accuracy, f1 = model.train(csv_file=args.input_csv, test_size=args.test_size)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model.save(args.output)

        print(f"\nüéâ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
        print(f"üìÅ –§–∞–π–ª –º–æ–¥–µ–ª–∏: {args.output}")
        print(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏: Accuracy={accuracy:.4f}, F1={f1:.4f}")

        # –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        print("\nüß™ –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:")
        test_query = "–ò–ò –≤ —Ç—Ä–µ–π–¥–∏–Ω–≥–µ"
        test_content = (
            "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∞–∫—Ç–∏–≤–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–º —Ç—Ä–µ–π–¥–∏–Ω–≥–µ..."
        )
        prediction, probability = model.predict(test_query, test_content)
        print(f"   –ó–∞–ø—Ä–æ—Å: '{test_query}'")
        print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {prediction} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {probability:.4f})")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()
