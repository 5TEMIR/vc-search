import time
from typing import List, Dict, Tuple
from ..scraper.vc_scraper import VCScraper
from ..storage.json_storage import JSONStorage
from ..models.article import Article
from ..config import ScrapingConfig


class ScrapingService:
    def __init__(self, config: ScrapingConfig):
        self.sections = config.sections
        self.articles_per_section = config.articles_per_section
        self.delay = config.delay
        self.batch_size = config.batch_size
        self.scraper = VCScraper(delay=config.delay, headless=config.headless)
        self.storage = JSONStorage()

    def scrape_section(self, section: str) -> Tuple[int, float, float]:
        """–°–∫—Ä–∞–ø–∏—Ç –æ–¥–∏–Ω —Ä–∞–∑–¥–µ–ª"""
        print(f"\nüéØ –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞–∑–¥–µ–ª: {section.upper()}")

        # –°–±–æ—Ä URL
        start_time = time.time()
        urls = self.scraper.get_article_urls_from_section(
            section, max_articles=self.articles_per_section
        )
        url_collection_time = time.time() - start_time

        print(
            f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∑–∞ {url_collection_time:.1f} —Å–µ–∫"
        )

        if not urls:
            print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–∑–¥–µ–ª {section} - –Ω–µ—Ç —Å—Ç–∞—Ç–µ–π")
            return 0, url_collection_time, 0

        # –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π –±–∞—Ç—á–∞–º–∏
        print(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ {len(urls)} —Å—Ç–∞—Ç–µ–π...")
        start_parse_time = time.time()
        total_parsed = 0

        for batch_start in range(0, len(urls), self.batch_size):
            batch_end = batch_start + self.batch_size
            batch_urls = urls[batch_start:batch_end]

            batch_articles = []
            for url in batch_urls:
                article = self.scraper.parse_article(url, section)
                if article:
                    batch_articles.append(article)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞—Ç—á
            saved_count = self.storage.save_articles_batch(batch_articles)
            total_parsed += saved_count

            print(
                f"üì¶ –ë–∞—Ç—á {batch_start // self.batch_size + 1}: "
                f"—Å–ø–∞—Ä—Å–µ–Ω–æ {len(batch_articles)}, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count}"
            )
            print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {min(batch_end, len(urls))}/{len(urls)} —Å—Ç–∞—Ç–µ–π")

        parse_time = time.time() - start_parse_time

        print(f"‚úÖ –†–∞–∑–¥–µ–ª {section.upper()} –∑–∞–≤–µ—Ä—à–µ–Ω:")
        print(f"   –°—Ç–∞—Ç–µ–π: {total_parsed}")
        print(f"   –í—Ä–µ–º—è —Å–±–æ—Ä–∞ URL: {url_collection_time:.1f} —Å–µ–∫")
        print(f"   –í—Ä–µ–º—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {parse_time:.1f} —Å–µ–∫")

        return total_parsed, url_collection_time, parse_time

    def scrape_all_sections(self) -> Dict:
        """–°–∫—Ä–∞–ø–∏—Ç –≤—Å–µ —Ä–∞–∑–¥–µ–ª—ã"""
        initial_count = self.storage.get_article_count()
        print(f"üìä –ù–∞—á–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π: {initial_count}")

        total_stats = {
            "total_articles": 0,
            "total_url_time": 0,
            "total_parse_time": 0,
            "section_stats": {},
        }

        for section in self.sections:
            try:
                articles_count, url_time, parse_time = self.scrape_section(section)

                total_stats["total_articles"] += articles_count
                total_stats["total_url_time"] += url_time
                total_stats["total_parse_time"] += parse_time
                total_stats["section_stats"][section] = {
                    "articles": articles_count,
                    "url_time": url_time,
                    "parse_time": parse_time,
                }

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ä–∞–∑–¥–µ–ª–µ {section}: {e}")
                continue

        final_count = self.storage.get_article_count()
        print(f"üìä –§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π: {final_count}")
        print(f"üìà –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π: {final_count - initial_count}")

        return total_stats

    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã"""
        if self.scraper:
            self.scraper.close()
